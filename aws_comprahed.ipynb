{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Comprehend\n",
    "\n",
    "Amazon Comprehend is a natural language processing (NLP) service that uses machine learning to find insights and relationships in text. You can also use AutoML capabilities in Amazon Comprehend to build a custom set of entities or text classification models that are tailored uniquely to your organization's needs.\n",
    "\n",
    "## Configuring Credentials\n",
    "\n",
    "There are two types of configuration data in boto3: credentials and non-credentials. Credentials include items such as aws_access_key_id, aws_secret_access_key, and aws_session_token. Non-credential configuration includes items such as which region to use or which addressing style to use for Amazon S3. The distinction between credentials and non-credentials configuration is important because the lookup process is slightly different. Boto3 will look in several additional locations when searching for credentials that do not apply when searching for non-credential configuration.\n",
    "\n",
    "The mechanism in which boto3 looks for credentials is to search through a list of possible locations and stop as soon as it finds credentials. The order in which Boto3 searches for credentials is:\n",
    "\n",
    "* Passing credentials as parameters in the **boto.client()** method\n",
    "* Passing credentials as parameters when creating a **Session** object\n",
    "* Environment variables\n",
    "* Shared credential file (~/.aws/credentials)\n",
    "* AWS config file (~/.aws/config)\n",
    "* Assume Role provider\n",
    "* Boto2 config file (/etc/boto.cfg and ~/.boto)\n",
    "* Instance metadata service on an Amazon EC2 instance that has an IAM role configured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples to initialize boto3\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY,\n",
    "    aws_session_token=SESSION_TOKEN,\n",
    ")\n",
    "\n",
    "# Or via the Session\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY,\n",
    "    aws_session_token=SESSION_TOKEN,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Packages\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from botocore import exceptions\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read credentials from file\n",
    "with open('aws_credentials.json') as json_file:\n",
    "    creds = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scion-dev\n"
     ]
    }
   ],
   "source": [
    "# get s3 object \n",
    "# credentials from system environments\n",
    "s3_client = boto3.client('s3',\n",
    "    aws_access_key_id=creds['AWSAccessKeyId'],\n",
    "    aws_secret_access_key=creds['AWSSecretKey'])\n",
    "s3 = boto3.resource('s3',\n",
    "    aws_access_key_id=creds['AWSAccessKeyId'],\n",
    "    aws_secret_access_key=creds['AWSSecretKey'])\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comprehend session\n",
    "comprehend = boto3.client(service_name='comprehend', region_name='REGION_CODE',\n",
    "                          aws_access_key_id=creds['AWSAccessKeyId'],\n",
    "                          aws_secret_access_key=creds['AWSSecretKey'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehend IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_access_role_arn = \"arn:aws:iam::Account_ID:role/service-role/AmazonComprehendServiceRole-Comprehend_Role\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DetectDominantLanguage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling DetectDominantLanguage\n",
      "{\n",
      "    \"Languages\": [\n",
      "        {\n",
      "            \"LanguageCode\": \"en\",\n",
      "            \"Score\": 0.9958581924438477\n",
      "        }\n",
      "    ],\n",
      "    \"ResponseMetadata\": {\n",
      "        \"HTTPHeaders\": {\n",
      "            \"content-length\": \"64\",\n",
      "            \"content-type\": \"application/x-amz-json-1.1\",\n",
      "            \"date\": \"Thu, 02 Jul 2020 06:39:45 GMT\",\n",
      "            \"x-amzn-requestid\": \"c36543af-f42b-4ca7-99c7-20c951aa4496\"\n",
      "        },\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"RequestId\": \"c36543af-f42b-4ca7-99c7-20c951aa4496\",\n",
      "        \"RetryAttempts\": 0\n",
      "    }\n",
      "}\n",
      "End of DetectDominantLanguage\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"It is raining today in Seattle\"\n",
    "\n",
    "print('Calling DetectDominantLanguage')\n",
    "print(json.dumps(comprehend.detect_dominant_language(Text = text), sort_keys=True, indent=4))\n",
    "print(\"End of DetectDominantLanguage\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling DetectEntities\n",
      "{\n",
      "    \"Entities\": [\n",
      "        {\n",
      "            \"BeginOffset\": 14,\n",
      "            \"EndOffset\": 19,\n",
      "            \"Score\": 0.9999421834945679,\n",
      "            \"Text\": \"today\",\n",
      "            \"Type\": \"DATE\"\n",
      "        },\n",
      "        {\n",
      "            \"BeginOffset\": 23,\n",
      "            \"EndOffset\": 30,\n",
      "            \"Score\": 0.999826967716217,\n",
      "            \"Text\": \"Seattle\",\n",
      "            \"Type\": \"LOCATION\"\n",
      "        }\n",
      "    ],\n",
      "    \"ResponseMetadata\": {\n",
      "        \"HTTPHeaders\": {\n",
      "            \"content-length\": \"199\",\n",
      "            \"content-type\": \"application/x-amz-json-1.1\",\n",
      "            \"date\": \"Thu, 02 Jul 2020 06:39:46 GMT\",\n",
      "            \"x-amzn-requestid\": \"cb5b1f0f-2fc5-408d-90d0-d6b60dcbfc57\"\n",
      "        },\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"RequestId\": \"cb5b1f0f-2fc5-408d-90d0-d6b60dcbfc57\",\n",
      "        \"RetryAttempts\": 0\n",
      "    }\n",
      "}\n",
      "End of DetectEntities\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"It is raining today in Seattle\"\n",
    "\n",
    "print('Calling DetectEntities')\n",
    "print(json.dumps(comprehend.detect_entities(Text=text, LanguageCode='en'), sort_keys=True, indent=4))\n",
    "print('End of DetectEntities\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Key Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling DetectKeyPhrases\n",
      "{\n",
      "    \"KeyPhrases\": [\n",
      "        {\n",
      "            \"BeginOffset\": 14,\n",
      "            \"EndOffset\": 19,\n",
      "            \"Score\": 1.0,\n",
      "            \"Text\": \"today\"\n",
      "        }\n",
      "    ],\n",
      "    \"ResponseMetadata\": {\n",
      "        \"HTTPHeaders\": {\n",
      "            \"content-length\": \"77\",\n",
      "            \"content-type\": \"application/x-amz-json-1.1\",\n",
      "            \"date\": \"Thu, 02 Jul 2020 06:39:46 GMT\",\n",
      "            \"x-amzn-requestid\": \"ccda5d22-8a62-4294-b76b-0a7e24ee78ec\"\n",
      "        },\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"RequestId\": \"ccda5d22-8a62-4294-b76b-0a7e24ee78ec\",\n",
      "        \"RetryAttempts\": 0\n",
      "    }\n",
      "}\n",
      "End of DetectKeyPhrases\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"It is raining today in Seattle\"\n",
    "\n",
    "print('Calling DetectKeyPhrases')\n",
    "print(json.dumps(comprehend.detect_key_phrases(Text=text, LanguageCode='en'), sort_keys=True, indent=4))\n",
    "print('End of DetectKeyPhrases\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling DetectSentiment\n",
      "{\n",
      "    \"ResponseMetadata\": {\n",
      "        \"HTTPHeaders\": {\n",
      "            \"content-length\": \"161\",\n",
      "            \"content-type\": \"application/x-amz-json-1.1\",\n",
      "            \"date\": \"Thu, 02 Jul 2020 06:39:48 GMT\",\n",
      "            \"x-amzn-requestid\": \"ba74d61c-15ef-4ec7-82f3-0d855cce6616\"\n",
      "        },\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"RequestId\": \"ba74d61c-15ef-4ec7-82f3-0d855cce6616\",\n",
      "        \"RetryAttempts\": 0\n",
      "    },\n",
      "    \"Sentiment\": \"NEUTRAL\",\n",
      "    \"SentimentScore\": {\n",
      "        \"Mixed\": 0.00021913634554948658,\n",
      "        \"Negative\": 0.162128284573555,\n",
      "        \"Neutral\": 0.7376415133476257,\n",
      "        \"Positive\": 0.10001111775636673\n",
      "    }\n",
      "}\n",
      "End of DetectSentiment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"It is raining today in Seattle\"\n",
    "\n",
    "print('Calling DetectSentiment')\n",
    "print(json.dumps(comprehend.detect_sentiment(Text=text, LanguageCode='en'), sort_keys=True, indent=4))\n",
    "print('End of DetectSentiment\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling DetectSyntax\n",
      "{\n",
      "    \"ResponseMetadata\": {\n",
      "        \"HTTPHeaders\": {\n",
      "            \"content-length\": \"714\",\n",
      "            \"content-type\": \"application/x-amz-json-1.1\",\n",
      "            \"date\": \"Thu, 02 Jul 2020 06:39:48 GMT\",\n",
      "            \"x-amzn-requestid\": \"945d6348-20ae-459c-86e7-db9e942899fc\"\n",
      "        },\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"RequestId\": \"945d6348-20ae-459c-86e7-db9e942899fc\",\n",
      "        \"RetryAttempts\": 0\n",
      "    },\n",
      "    \"SyntaxTokens\": [\n",
      "        {\n",
      "            \"BeginOffset\": 0,\n",
      "            \"EndOffset\": 2,\n",
      "            \"PartOfSpeech\": {\n",
      "                \"Score\": 0.9999788999557495,\n",
      "                \"Tag\": \"PRON\"\n",
      "            },\n",
      "            \"Text\": \"It\",\n",
      "            \"TokenId\": 1\n",
      "        },\n",
      "        {\n",
      "            \"BeginOffset\": 3,\n",
      "            \"EndOffset\": 5,\n",
      "            \"PartOfSpeech\": {\n",
      "                \"Score\": 0.9020146131515503,\n",
      "                \"Tag\": \"AUX\"\n",
      "            },\n",
      "            \"Text\": \"is\",\n",
      "            \"TokenId\": 2\n",
      "        },\n",
      "        {\n",
      "            \"BeginOffset\": 6,\n",
      "            \"EndOffset\": 13,\n",
      "            \"PartOfSpeech\": {\n",
      "                \"Score\": 0.998913049697876,\n",
      "                \"Tag\": \"VERB\"\n",
      "            },\n",
      "            \"Text\": \"raining\",\n",
      "            \"TokenId\": 3\n",
      "        },\n",
      "        {\n",
      "            \"BeginOffset\": 14,\n",
      "            \"EndOffset\": 19,\n",
      "            \"PartOfSpeech\": {\n",
      "                \"Score\": 0.9994751811027527,\n",
      "                \"Tag\": \"NOUN\"\n",
      "            },\n",
      "            \"Text\": \"today\",\n",
      "            \"TokenId\": 4\n",
      "        },\n",
      "        {\n",
      "            \"BeginOffset\": 20,\n",
      "            \"EndOffset\": 22,\n",
      "            \"PartOfSpeech\": {\n",
      "                \"Score\": 0.9998569488525391,\n",
      "                \"Tag\": \"ADP\"\n",
      "            },\n",
      "            \"Text\": \"in\",\n",
      "            \"TokenId\": 5\n",
      "        },\n",
      "        {\n",
      "            \"BeginOffset\": 23,\n",
      "            \"EndOffset\": 30,\n",
      "            \"PartOfSpeech\": {\n",
      "                \"Score\": 0.9937676191329956,\n",
      "                \"Tag\": \"PROPN\"\n",
      "            },\n",
      "            \"Text\": \"Seattle\",\n",
      "            \"TokenId\": 6\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "End of DetectSyntax\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"It is raining today in Seattle\"\n",
    " \n",
    "print('Calling DetectSyntax')\n",
    "print(json.dumps(comprehend.detect_syntax(Text=text, LanguageCode='en'), sort_keys=True, indent=4))\n",
    "print('End of DetectSyntax\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Custom Jobs and Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Detection Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_topics_detection_job_result: {\"JobId\": \"216a5699a96627feb93dad9ab9c3bd46\", \"JobStatus\": \"SUBMITTED\", \"ResponseMetadata\": {\"RequestId\": \"543ec08b-1b96-4e6a-a95f-e4f38afff25f\", \"HTTPStatusCode\": 200, \"HTTPHeaders\": {\"x-amzn-requestid\": \"543ec08b-1b96-4e6a-a95f-e4f38afff25f\", \"content-type\": \"application/x-amz-json-1.1\", \"content-length\": \"68\", \"date\": \"Thu, 02 Jul 2020 06:41:38 GMT\"}, \"RetryAttempts\": 0}}\n"
     ]
    }
   ],
   "source": [
    "# start_topics_detection_job \n",
    "\n",
    "input_s3_url = \"s3://BUCKET_NAME/input_data/topics_data.csv\"\n",
    "input_doc_format = \"ONE_DOC_PER_FILE\"\n",
    "output_s3_url = \"s3://BUCKET_NAME/job_output\"\n",
    "\n",
    "number_of_topics = 10\n",
    " \n",
    "input_data_config = {\"S3Uri\": input_s3_url, \"InputFormat\": input_doc_format}\n",
    "output_data_config = {\"S3Uri\": output_s3_url}\n",
    " \n",
    "start_topics_detection_job_result = comprehend.start_topics_detection_job(JobName=\"Topics\",NumberOfTopics=number_of_topics,\n",
    "                                                                              InputDataConfig=input_data_config,\n",
    "                                                                              OutputDataConfig=output_data_config,\n",
    "                                                                              DataAccessRoleArn=data_access_role_arn)\n",
    " \n",
    "print('start_topics_detection_job_result: ' + json.dumps(start_topics_detection_job_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_id: 216a5699a96627feb93dad9ab9c3bd46\n"
     ]
    }
   ],
   "source": [
    "job_id = start_topics_detection_job_result[\"JobId\"]\n",
    " \n",
    "print('job_id: ' + job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bson import json_util\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "while True:\n",
    "    describe_topics_detection_job_result = comprehend.describe_topics_detection_job(JobId=job_id)\n",
    "    if describe_topics_detection_job_result['TopicsDetectionJobProperties']['JobStatus'] == \"COMPLETED\":\n",
    "        break\n",
    "    print(\"Not ready yet...\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "describe_topics_detection_job_result['TopicsDetectionJobProperties']['SubmitTime'] = str(describe_topics_detection_job_result['TopicsDetectionJobProperties']['SubmitTime'].strftime('%m/%d/%Y %H:%M:%S.%f'))\n",
    "\n",
    "describe_topics_detection_job_result['TopicsDetectionJobProperties']['EndTime'] = str(describe_topics_detection_job_result['TopicsDetectionJobProperties']['EndTime'].strftime('%m/%d/%Y %H:%M:%S.%f'))\n",
    " \n",
    "pprint('describe_topics_detection_job_result: ' + json.dumps(describe_topics_detection_job_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ouput data '\n",
      " 'location:s3://scion-dev/job_output/952449798557-TOPICS-216a5699a96627feb93dad9ab9c3bd46/output/output.tar.gz')\n"
     ]
    }
   ],
   "source": [
    "output_data_location = describe_topics_detection_job_result['TopicsDetectionJobProperties']['OutputDataConfig']['S3Uri']\n",
    "pprint(\"Ouput data location:\"+output_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from s3\n",
    "bucket_path = f\"job_output/{'/'.join(output_data_location.split('/')[4:])}\"\n",
    "with open(r'C:\\Users\\USER\\output\\topics.tar.gz', 'wb') as f:\n",
    "    s3_client.download_fileobj(\"scion-dev\", bucket_path, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract downloaded file\n",
    "import shutil\n",
    "shutil.unpack_archive(r'C:\\Users\\USER\\output\\topics.tar.gz', \n",
    "                      r'C:\\Users\\USER\\output\\topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files from extracted folder\n",
    "import glob\n",
    "\n",
    "extracted_files = glob.glob(r'C:\\Users\\USER\\output\\topics\\*')\n",
    "print(extracted_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Load topics \n",
    "import pandas as pd\n",
    "\n",
    "doc_topics_df = pd.read_csv(extracted_files[0])\n",
    "topic_terms_df = pd.read_csv(extracted_files[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Topics from Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docname</th>\n",
       "      <th>topic</th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>topics_data.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0.379628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>topics_data.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>0.225014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>topics_data.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>0.201156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>topics_data.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>0.161101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>topics_data.csv</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           docname  topic  proportion\n",
       "0  topics_data.csv      0    0.379628\n",
       "1  topics_data.csv      1    0.225014\n",
       "2  topics_data.csv      2    0.201156\n",
       "3  topics_data.csv      3    0.161101\n",
       "4  topics_data.csv      4    0.033101"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topics_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topics from Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>term</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>patchy</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>alan</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>editorial</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>torture</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>robbins</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>mirco</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>fair</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>controller</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>phase</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>allot</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>patchy</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>alan</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>editorial</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>torture</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>robbins</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>mirco</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>fair</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>controller</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>phase</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>allot</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>patchy</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>alan</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>editorial</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>torture</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>robbins</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>mirco</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>fair</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>controller</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>phase</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>allot</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>patchy</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>alan</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>editorial</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>torture</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3</td>\n",
       "      <td>robbins</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>mirco</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3</td>\n",
       "      <td>fair</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>controller</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>phase</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>allot</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>patchy</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4</td>\n",
       "      <td>alan</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>editorial</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>torture</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4</td>\n",
       "      <td>robbins</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4</td>\n",
       "      <td>mirco</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4</td>\n",
       "      <td>fair</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4</td>\n",
       "      <td>controller</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4</td>\n",
       "      <td>phase</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4</td>\n",
       "      <td>allot</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic        term    weight\n",
       "0       0      patchy  0.000484\n",
       "1       0        alan  0.000484\n",
       "2       0   editorial  0.000484\n",
       "3       0     torture  0.000484\n",
       "4       0     robbins  0.000484\n",
       "5       0       mirco  0.000484\n",
       "6       0        fair  0.000484\n",
       "7       0  controller  0.000484\n",
       "8       0       phase  0.000484\n",
       "9       0       allot  0.000484\n",
       "10      1      patchy  0.000484\n",
       "11      1        alan  0.000484\n",
       "12      1   editorial  0.000484\n",
       "13      1     torture  0.000484\n",
       "14      1     robbins  0.000484\n",
       "15      1       mirco  0.000484\n",
       "16      1        fair  0.000484\n",
       "17      1  controller  0.000484\n",
       "18      1       phase  0.000484\n",
       "19      1       allot  0.000484\n",
       "20      2      patchy  0.000484\n",
       "21      2        alan  0.000484\n",
       "22      2   editorial  0.000484\n",
       "23      2     torture  0.000484\n",
       "24      2     robbins  0.000484\n",
       "25      2       mirco  0.000484\n",
       "26      2        fair  0.000484\n",
       "27      2  controller  0.000484\n",
       "28      2       phase  0.000484\n",
       "29      2       allot  0.000484\n",
       "30      3      patchy  0.000484\n",
       "31      3        alan  0.000484\n",
       "32      3   editorial  0.000484\n",
       "33      3     torture  0.000484\n",
       "34      3     robbins  0.000484\n",
       "35      3       mirco  0.000484\n",
       "36      3        fair  0.000484\n",
       "37      3  controller  0.000484\n",
       "38      3       phase  0.000484\n",
       "39      3       allot  0.000484\n",
       "40      4      patchy  0.000484\n",
       "41      4        alan  0.000484\n",
       "42      4   editorial  0.000484\n",
       "43      4     torture  0.000484\n",
       "44      4     robbins  0.000484\n",
       "45      4       mirco  0.000484\n",
       "46      4        fair  0.000484\n",
       "47      4  controller  0.000484\n",
       "48      4       phase  0.000484\n",
       "49      4       allot  0.000484"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_terms_df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing existing TopicDetection Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_topics_detection_jobs = comprehend.list_topics_detection_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_topics_detection_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train data\n",
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(r\"C:\\Users\\USER\\bbc-text\\bbc-text.csv\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Traning data format\n",
    "* Headrs should removed\n",
    "* The class name is placed first, followed by the complete document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample MultiClass classification with custom models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model using Custom Document Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document classifier\n",
    "create_response = comprehend.create_document_classifier(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': 's3://BUCKET_NAME/input_data/train.csv'\n",
    "    },\n",
    "    DataAccessRoleArn=data_access_role_arn,\n",
    "    DocumentClassifierName='SampleCodeClassifier1',\n",
    "    LanguageCode='en'\n",
    ")\n",
    "print(\"Create response: %s\\n\", create_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_response = comprehend.describe_document_classifier(\n",
    "    DocumentClassifierArn=create_response['DocumentClassifierArn'])\n",
    "describe_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status of the classifier\n",
    "\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "# 'Status': 'SUBMITTED'|'TRAINING'|'DELETING'|'STOP_REQUESTED'|'STOPPED'|'IN_ERROR'|'TRAINED'\n",
    "while True:\n",
    "    describe_response = comprehend.describe_document_classifier(\n",
    "    DocumentClassifierArn=create_response['DocumentClassifierArn'])\n",
    "    if describe_response['DocumentClassifierProperties']['Status'] == \"TRAINED\":\n",
    "        break\n",
    "    print(\"Not ready yet...\")\n",
    "    time.sleep(300)\n",
    "    \n",
    "print(\"Describe response: %s\\n\", describe_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all classifiers in account\n",
    "list_response = comprehend.list_document_classifiers()\n",
    "print(\"List response: %s\\n\", list_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a classifier Job to predict labels using created custom classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start response: %s\n",
      " {'JobId': 'ffa665fbe20c3d0a4340fa42ce0224ec', 'JobStatus': 'SUBMITTED', 'ResponseMetadata': {'RequestId': '7efb3e2f-806d-465e-bb5c-1ec8563ac705', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '7efb3e2f-806d-465e-bb5c-1ec8563ac705', 'content-type': 'application/x-amz-json-1.1', 'content-length': '68', 'date': 'Thu, 02 Jul 2020 09:38:48 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "start_response = comprehend.start_document_classification_job(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': 's3://BUCKET_NAME/input_data/test.csv',\n",
    "        'InputFormat': 'ONE_DOC_PER_LINE'\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': 's3://BUCKET_NAME/job_output'\n",
    "    },\n",
    "    DataAccessRoleArn=data_access_role_arn,\n",
    "    DocumentClassifierArn=\n",
    "    'arn:aws:comprehend:ap-south-1:ACCOUNT_ID:document-classifier/SampleCodeClassifier1'\n",
    ")\n",
    "\n",
    "print(\"Start response: %s\\n\", start_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bson import json_util\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "# Check the status of the job\n",
    "while True:\n",
    "    describe_response = comprehend.describe_document_classification_job(JobId=start_response['JobId'])\n",
    "    if describe_response['DocumentClassificationJobProperties']['JobStatus'] == \"COMPLETED\":\n",
    "        break\n",
    "    print(\"Not ready yet...\")\n",
    "    time.sleep(200)\n",
    "\n",
    "describe_response['DocumentClassificationJobProperties']['SubmitTime'] = str(describe_response['DocumentClassificationJobProperties']['SubmitTime'].strftime('%m/%d/%Y %H:%M:%S.%f'))\n",
    "\n",
    "describe_response['DocumentClassificationJobProperties']['EndTime'] = str(describe_response['DocumentClassificationJobProperties']['EndTime'].strftime('%m/%d/%Y %H:%M:%S.%f'))\n",
    " \n",
    "pprint('describe_document_classification_job_result: ' + json.dumps(describe_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_output_data_location = describe_response['DocumentClassificationJobProperties']['OutputDataConfig']['S3Uri']\n",
    "pprint(\"Ouput data location:\"+classifier_output_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from s3\n",
    "bucket_path = f\"job_output/{'/'.join(classifier_output_data_location.split('/')[4:])}\"\n",
    "with open(r'C:\\Users\\USER\\output\\classifier.tar.gz', 'wb') as f:\n",
    "    s3_client.download_fileobj(\"scion-dev\", bucket_path, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract downloaded file\n",
    "import shutil\n",
    "shutil.unpack_archive(r'C:\\Users\\USER\\output\\classifier.tar.gz', \n",
    "                      r'C:\\Users\\USER\\output\\classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files from extracted folder\n",
    "import glob\n",
    "\n",
    "extracted_files = glob.glob(r'C:\\Users\\USER\\output\\classifier\\*')\n",
    "print(extracted_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"File\": \"test.csv\", \"Line\": \"0\", \"Classes\": [{\"Name\": \"business\", \"Score\": '\n",
      " '0.9735}, {\"Name\": \"entertainment\", \"Score\": 0.013}, {\"Name\": \"tech\", '\n",
      " '\"Score\": 0.0059}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"1\", \"Classes\": [{\"Name\": \"tech\", \"Score\": '\n",
      " '0.9586}, {\"Name\": \"entertainment\", \"Score\": 0.0318}, {\"Name\": \"politics\", '\n",
      " '\"Score\": 0.0054}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"2\", \"Classes\": [{\"Name\": \"politics\", \"Score\": '\n",
      " '0.7099}, {\"Name\": \"business\", \"Score\": 0.2326}, {\"Name\": \"tech\", \"Score\": '\n",
      " '0.03}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"3\", \"Classes\": [{\"Name\": \"tech\", \"Score\": '\n",
      " '0.9539}, {\"Name\": \"entertainment\", \"Score\": 0.0321}, {\"Name\": \"politics\", '\n",
      " '\"Score\": 0.0061}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"4\", \"Classes\": [{\"Name\": \"politics\", \"Score\": '\n",
      " '0.8461}, {\"Name\": \"entertainment\", \"Score\": 0.0932}, {\"Name\": \"business\", '\n",
      " '\"Score\": 0.0425}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"5\", \"Classes\": [{\"Name\": \"tech\", \"Score\": '\n",
      " '0.8239}, {\"Name\": \"business\", \"Score\": 0.1514}, {\"Name\": \"entertainment\", '\n",
      " '\"Score\": 0.0141}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"6\", \"Classes\": [{\"Name\": \"entertainment\", '\n",
      " '\"Score\": 0.9798}, {\"Name\": \"sport\", \"Score\": 0.0111}, {\"Name\": \"politics\", '\n",
      " '\"Score\": 0.0046}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"7\", \"Classes\": [{\"Name\": \"politics\", \"Score\": '\n",
      " '0.9999}, {\"Name\": \"entertainment\", \"Score\": 0.0}, {\"Name\": \"tech\", \"Score\": '\n",
      " '0.0}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"8\", \"Classes\": [{\"Name\": \"tech\", \"Score\": '\n",
      " '1.0}, {\"Name\": \"business\", \"Score\": 0.0}, {\"Name\": \"entertainment\", \"Score\": '\n",
      " '0.0}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"9\", \"Classes\": [{\"Name\": \"entertainment\", '\n",
      " '\"Score\": 0.9955}, {\"Name\": \"business\", \"Score\": 0.0016}, {\"Name\": \"tech\", '\n",
      " '\"Score\": 0.0012}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"10\", \"Classes\": [{\"Name\": \"sport\", \"Score\": '\n",
      " '0.9988}, {\"Name\": \"tech\", \"Score\": 0.0006}, {\"Name\": \"politics\", \"Score\": '\n",
      " '0.0004}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"11\", \"Classes\": [{\"Name\": \"politics\", \"Score\": '\n",
      " '0.8765}, {\"Name\": \"business\", \"Score\": 0.0609}, {\"Name\": \"entertainment\", '\n",
      " '\"Score\": 0.025}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"12\", \"Classes\": [{\"Name\": \"entertainment\", '\n",
      " '\"Score\": 0.9855}, {\"Name\": \"sport\", \"Score\": 0.0116}, {\"Name\": \"politics\", '\n",
      " '\"Score\": 0.0025}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"13\", \"Classes\": [{\"Name\": \"entertainment\", '\n",
      " '\"Score\": 0.3841}, {\"Name\": \"business\", \"Score\": 0.2588}, {\"Name\": \"tech\", '\n",
      " '\"Score\": 0.1517}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"14\", \"Classes\": [{\"Name\": \"tech\", \"Score\": '\n",
      " '0.9999}, {\"Name\": \"entertainment\", \"Score\": 0.0}, {\"Name\": \"business\", '\n",
      " '\"Score\": 0.0}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"15\", \"Classes\": [{\"Name\": \"business\", \"Score\": '\n",
      " '1.0}, {\"Name\": \"politics\", \"Score\": 0.0}, {\"Name\": \"tech\", \"Score\": 0.0}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"16\", \"Classes\": [{\"Name\": \"tech\", \"Score\": '\n",
      " '0.9998}, {\"Name\": \"business\", \"Score\": 0.0001}, {\"Name\": \"entertainment\", '\n",
      " '\"Score\": 0.0}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"17\", \"Classes\": [{\"Name\": \"entertainment\", '\n",
      " '\"Score\": 0.9997}, {\"Name\": \"politics\", \"Score\": 0.0001}, {\"Name\": \"tech\", '\n",
      " '\"Score\": 0.0001}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"18\", \"Classes\": [{\"Name\": \"tech\", \"Score\": '\n",
      " '1.0}, {\"Name\": \"politics\", \"Score\": 0.0}, {\"Name\": \"entertainment\", \"Score\": '\n",
      " '0.0}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"19\", \"Classes\": [{\"Name\": \"sport\", \"Score\": '\n",
      " '0.9745}, {\"Name\": \"entertainment\", \"Score\": 0.0118}, {\"Name\": \"politics\", '\n",
      " '\"Score\": 0.0078}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"20\", \"Classes\": [{\"Name\": \"business\", \"Score\": '\n",
      " '0.795}, {\"Name\": \"politics\", \"Score\": 0.0736}, {\"Name\": \"tech\", \"Score\": '\n",
      " '0.0612}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"21\", \"Classes\": [{\"Name\": \"business\", \"Score\": '\n",
      " '0.9962}, {\"Name\": \"entertainment\", \"Score\": 0.0018}, {\"Name\": \"tech\", '\n",
      " '\"Score\": 0.0009}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"22\", \"Classes\": [{\"Name\": \"politics\", \"Score\": '\n",
      " '0.9019}, {\"Name\": \"business\", \"Score\": 0.0414}, {\"Name\": \"entertainment\", '\n",
      " '\"Score\": 0.0362}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"23\", \"Classes\": [{\"Name\": \"entertainment\", '\n",
      " '\"Score\": 0.8964}, {\"Name\": \"sport\", \"Score\": 0.0368}, {\"Name\": \"business\", '\n",
      " '\"Score\": 0.0292}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"24\", \"Classes\": [{\"Name\": \"politics\", \"Score\": '\n",
      " '0.7667}, {\"Name\": \"tech\", \"Score\": 0.1456}, {\"Name\": \"entertainment\", '\n",
      " '\"Score\": 0.0629}]}\\n',\n",
      " '{\"File\": \"test.csv\", \"Line\": \"25\", \"Classes\": [{\"Name\": \"sport\", \"Score\": '\n",
      " '0.9875}, {\"Name\": \"entertainment\", \"Score\": 0.0066}, {\"Name\": \"politics\", '\n",
      " '\"Score\": 0.0037}]}\\n']\n"
     ]
    }
   ],
   "source": [
    "# read jsonl file\n",
    "with open(extracted_files[0],'r') as fp:\n",
    "    fdata = fp.readlines()\n",
    "pprint(fdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "pres = []\n",
    "for line in fdata:\n",
    "    da = json.loads(line)['Classes']\n",
    "    pres.append(da[0]['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data and preditions\n",
    "\n",
    "test_data = pd.read_csv(r\"C:\\Users\\USER\\bbc-text\\test.csv\",names = ['labels','docs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['preditions'] = pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>docs</th>\n",
       "      <th>preditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>wall street cool to ebay s profit shares in on...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tech</td>\n",
       "      <td>uk pioneers digital film network the world s f...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>ban on forced retirement under 65 employers wi...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tech</td>\n",
       "      <td>local net tv takes off in austria an austrian ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politics</td>\n",
       "      <td>profile: david miliband david miliband s rapid...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tech</td>\n",
       "      <td>argonaut founder rebuilds empire jez san  the ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>dance music not dead says fatboy dj norman coo...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>politics</td>\n",
       "      <td>kennedy questions trust of blair lib dem leade...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tech</td>\n",
       "      <td>california sets fines for spyware the makers o...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>snicket tops us box office chart the film adap...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sport</td>\n",
       "      <td>time to get tough on friendlies  for an intern...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>politics</td>\n",
       "      <td>teens  know little  of politics teenagers ques...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>lopez misses uk charity premiere jennifer lope...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>business</td>\n",
       "      <td>christmas shoppers flock to tills shops all ov...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tech</td>\n",
       "      <td>progress on new internet domains by early 2005...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>business</td>\n",
       "      <td>bush budget seeks deep cutbacks president bush...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tech</td>\n",
       "      <td>junk e-mails on relentless rise spam traffic i...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>top stars join us tsunami tv show brad pitt  r...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tech</td>\n",
       "      <td>rings of steel combat net attacks gambling is ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sport</td>\n",
       "      <td>davies favours gloucester future wales hooker ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>business</td>\n",
       "      <td>beijingers fume over parking fees choking traf...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>business</td>\n",
       "      <td>cars pull down us retail figures us retail sal...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>politics</td>\n",
       "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>rem announce new glasgow concert us band rem h...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>politics</td>\n",
       "      <td>how political squabbles snowball it s become c...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>sport</td>\n",
       "      <td>souness delight at euro progress boss graeme s...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           labels                                               docs  \\\n",
       "0        business  wall street cool to ebay s profit shares in on...   \n",
       "1            tech  uk pioneers digital film network the world s f...   \n",
       "2        business  ban on forced retirement under 65 employers wi...   \n",
       "3            tech  local net tv takes off in austria an austrian ...   \n",
       "4        politics  profile: david miliband david miliband s rapid...   \n",
       "5            tech  argonaut founder rebuilds empire jez san  the ...   \n",
       "6   entertainment  dance music not dead says fatboy dj norman coo...   \n",
       "7        politics  kennedy questions trust of blair lib dem leade...   \n",
       "8            tech  california sets fines for spyware the makers o...   \n",
       "9   entertainment  snicket tops us box office chart the film adap...   \n",
       "10          sport  time to get tough on friendlies  for an intern...   \n",
       "11       politics  teens  know little  of politics teenagers ques...   \n",
       "12  entertainment  lopez misses uk charity premiere jennifer lope...   \n",
       "13       business  christmas shoppers flock to tills shops all ov...   \n",
       "14           tech  progress on new internet domains by early 2005...   \n",
       "15       business  bush budget seeks deep cutbacks president bush...   \n",
       "16           tech  junk e-mails on relentless rise spam traffic i...   \n",
       "17  entertainment  top stars join us tsunami tv show brad pitt  r...   \n",
       "18           tech  rings of steel combat net attacks gambling is ...   \n",
       "19          sport  davies favours gloucester future wales hooker ...   \n",
       "20       business  beijingers fume over parking fees choking traf...   \n",
       "21       business  cars pull down us retail figures us retail sal...   \n",
       "22       politics  kilroy unveils immigration policy ex-chatshow ...   \n",
       "23  entertainment  rem announce new glasgow concert us band rem h...   \n",
       "24       politics  how political squabbles snowball it s become c...   \n",
       "25          sport  souness delight at euro progress boss graeme s...   \n",
       "\n",
       "       preditions  \n",
       "0        business  \n",
       "1            tech  \n",
       "2        politics  \n",
       "3            tech  \n",
       "4        politics  \n",
       "5            tech  \n",
       "6   entertainment  \n",
       "7        politics  \n",
       "8            tech  \n",
       "9   entertainment  \n",
       "10          sport  \n",
       "11       politics  \n",
       "12  entertainment  \n",
       "13  entertainment  \n",
       "14           tech  \n",
       "15       business  \n",
       "16           tech  \n",
       "17  entertainment  \n",
       "18           tech  \n",
       "19          sport  \n",
       "20       business  \n",
       "21       business  \n",
       "22       politics  \n",
       "23  entertainment  \n",
       "24       politics  \n",
       "25          sport  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all classification jobs in account\n",
    "list_response = comprehend.list_document_classification_jobs()\n",
    "print(\"List response: %s\\n\", list_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.aws.amazon.com/comprehend/latest/dg/get-started-api-med.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.aws.amazon.com/comprehend/latest/dg/get-started-customclass.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.aws.amazon.com/comprehend/latest/dg/get-started-topics.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification-training.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=p5vaikbltIk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/hervenivon/aws-experiments-comprehend-custom-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
